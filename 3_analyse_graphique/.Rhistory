}
# Stop the clock : affichera dans la console le temps d'execution du script
#(environ 15 minutes pour le site de la Wallonie)
Sys.time() - start_time
View(tables_completes)
urls_tables = "http://bru2018.brussels/fr/results/municipalities/6074/index.html"
html = urls_tables %>%
read_html()
candidats = html %>%
html_nodes(".col-xs-5") %>%
html_text() %>%
trimws()
voix = html %>%
html_nodes(".col-xs-2 div+ div") %>%
html_text() %>%
trimws()
elu = html %>%
html_nodes(".inline-block:nth-child(6)") %>%
html_text() %>%
trimws()
table = as_tibble(cbind(candidats, voix, elu))
#on crée trois nouvelles colonnes pour stocker le nom de la commune, le nom de la liste et l'URL de la page
table$commune = html %>% html_node(".col-xs-8 a") %>% html_text()
table$parti = html %>% html_node(".subtitle+ .title-print-info button") %>% html_text()
table$lien = urls_tables
View(table)
tables_completes = vector()
urls_tables = "http://bru2018.brussels/fr/results/municipalities/6074/index.html"
html = urls_tables %>%
read_html()
candidats = html %>%
html_nodes(".col-xs-5") %>%
html_text() %>%
trimws()
voix = html %>%
html_nodes(".col-xs-2 div+ div") %>%
html_text() %>%
trimws()
elu = html %>%
html_nodes(".inline-block:nth-child(6)") %>%
html_text() %>%
trimws()
table = as_tibble(cbind(candidats, voix, elu))
#on crée trois nouvelles colonnes pour stocker le nom de la commune, le nom de la liste et l'URL de la page
table$commune = html %>% html_node(".col-xs-8 a") %>% html_text()
table$parti = html %>% html_node(".subtitle+ .title-print-info button") %>% html_text()
table$lien = urls_tables
#on fusionne chaque nouvelle table dans un tableau global
tables_completes = rbind(tables_completes, table)
View(tables_completes)
html
html %>% html_node(".col-xs-8 a")
table$commune = html %>% html_node(".subtitle .inline-block")
table$commune = html %>% html_node(".subtitle .inline-block") %>% html_text()
table$commune
View(table)
tables_completes = vector()
urls_tables = "http://bru2018.brussels/fr/results/municipalities/6074/index.html"
html = urls_tables %>%
read_html()
candidats = html %>%
html_nodes(".col-xs-5") %>%
html_text() %>%
trimws()
voix = html %>%
html_nodes(".col-xs-2 div+ div") %>%
html_text() %>%
trimws()
elu = html %>%
html_nodes(".inline-block:nth-child(6)") %>%
html_text() %>%
trimws()
table = as_tibble(cbind(candidats, voix, elu))
#on crée trois nouvelles colonnes pour stocker le nom de la commune, le nom de la liste et l'URL de la page
table$commune = html %>% html_node(".subtitle .inline-block") %>% html_text()
table$parti = html %>% html_node(".subtitle+ .title-print-info button") %>% html_text()
table$lien = urls_tables
View(table)
candidats
candidats = html %>%
html_nodes(".col-xs-5") %>%
html_text() %>%
trimws()
candidats
urls_tables = "http://bru2018.brussels/fr/results/municipalities/6074/lists/241.html"
html = urls_tables %>%
read_html()
candidats = html %>%
html_nodes(".col-xs-5") %>%
html_text() %>%
trimws()
voix = html %>%
html_nodes(".col-xs-2 div+ div") %>%
html_text() %>%
trimws()
elu = html %>%
html_nodes(".inline-block:nth-child(6)") %>%
html_text() %>%
trimws()
table = as_tibble(cbind(candidats, voix, elu))
#on crée trois nouvelles colonnes pour stocker le nom de la commune, le nom de la liste et l'URL de la page
table$commune = html %>% html_node(".subtitle .inline-block") %>% html_text()
table$parti = html %>% html_node(".subtitle+ .title-print-info button") %>% html_text()
table$lien = urls_tables
View(table)
# Start the clock! Pour mesurer (par curiosité) le temps que prendra l'execution de ce script
start_time <- Sys.time()
library("rvest") #le package de scraping
library("tidyverse") #contient tout ce qu'il faut pour manipuler les données
#URL de la page contenant les liens vers chaque commune (page de départ)
start_url = "http://bru2018.brussels/fr/counting/index.html"
#Url de base du site
base_url = "http://bru2018.brussels"
#étape 1 : On récupère les urls des communes
url_communes =
start_url %>%
read_html() %>%
#le sélecteur CSS a été retrouvé
#grâce au selector gadget de Chrome
html_nodes(".title a") %>%
html_attr("href") %>%
paste0(base_url, .)
#étape 2 : On récupère les URLs des listes électorales de chaque commune
listes = vector()
for (url in url_communes) {
urls_listes = url %>%
read_html() %>%
html_nodes(".candidats a") %>%
html_attr("href") %>%
paste0(base_url, .)
listes = c(listes, urls_listes)
}
#étape 3 : On récupère les tableaux de résultats qu'on stocke dans une seule liste
tables_completes = vector()
for(urls_tables in listes){
html = urls_tables %>%
read_html()
candidats = html %>%
html_nodes(".col-xs-5") %>%
html_text() %>%
trimws()
voix = html %>%
html_nodes(".col-xs-2 div+ div") %>%
html_text() %>%
trimws()
elu = html %>%
html_nodes(".inline-block:nth-child(6)") %>%
html_text() %>%
trimws()
table = as_tibble(cbind(candidats, voix, elu))
#on crée trois nouvelles colonnes pour stocker le nom de la commune, le nom de la liste et l'URL de la page
table$commune = html %>% html_node(".subtitle .inline-block") %>% html_text()
table$parti = html %>% html_node(".subtitle+ .title-print-info button") %>% html_text()
table$lien = urls_tables
#on fusionne chaque nouvelle table dans un tableau global
tables_completes = rbind(tables_completes, table)
}
# Stop the clock : affichera dans la console le temps d'execution du script
#(environ 15 minutes pour le site de la Wallonie)
Sys.time() - start_time
#########################################################################
#Pas le temps de nettoyer les résultats automatiquement
#à faire dans OpenRefine
#vérifier s'il y a bien 4104 candidats à BXL
#########################################################################
#étape 4 : On sauvegarde les résultats dans un csv "Excel compatible" (vous n'aurez pas de symboles bizarres à la place des lettres accentuées)
write_excel_csv(tables_completes, "scraping_2018_bruxelles.csv")
View(tables_completes)
# Start the clock! Pour mesurer (par curiosité) le temps que prendra l'execution de ce script
start_time <- Sys.time()
library("rvest") #le package de scraping
library("tidyverse") #contient tout ce qu'il faut pour manipuler les données
#URL de la page contenant les liens vers chaque commune (page de départ)
start_url = "https://elections2018.wallonie.be/fr/election?el=CG"
#Url de base du site
base_url = "https://elections2018.wallonie.be"
#étape 1 : On récupère les urls des communes
url_communes =
start_url %>%
read_html() %>%
#le sélecteur CSS "#block-blockelectionstart a" a été retrouvé (après un peu de chipo...)
#grâce au selector gadget de Chrome
html_nodes("#block-blockelectionstart a") %>%
html_attr("href") %>%
paste0(base_url, .)
url_communes =
start_url %>%
read_html() %>%
#le sélecteur CSS "#block-blockelectionstart a" a été retrouvé (après un peu de chipo...)
#grâce au selector gadget de Chrome
html_nodes(".definition__header") %>%
html_attr("href") %>%
paste0(base_url, .)
library(readr)
scraping_2018_wallonie_bruxelles_communales <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
library(dplyr)
library(readr)
library(ggplot2)
#ne faite pas attention à ces lignes de code
#elles servent à définir automatiquement votre
#répertoire de travail dans le bon dossier
set_wd <- function() {
library(rstudioapi) # à installer au besoin
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path ))
print( getwd() )
}
set_wd()
#On importe et visualise le fichier csv issu des scraping
scraping_2018_wallonie_bruxelles_communales <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(voix[which(elu=="oui")]),
difference = voix - voix_last_elu) %>%
filter(elu=="non", difference==-1)
View(rate_lelection)
library(dplyr)
library(readr)
library(ggplot2)
#ne faite pas attention à ces lignes de code
#elles servent à définir automatiquement votre
#répertoire de travail dans le bon dossier
set_wd <- function() {
library(rstudioapi) # à installer au besoin
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path ))
print( getwd() )
}
set_wd()
#On importe et visualise le fichier csv issu des scraping
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(voix[which(elu=="oui")]),
difference = voix - voix_last_elu) %>%
filter(elu=="non", difference==-2)
View(rate_lelection)
library(dplyr)
library(readr)
library(ggplot2)
#ne faite pas attention à ces lignes de code
#elles servent à définir automatiquement votre
#répertoire de travail dans le bon dossier
set_wd <- function() {
library(rstudioapi) # à installer au besoin
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path ))
print( getwd() )
}
set_wd()
#On importe et visualise le fichier csv issu des scraping
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
library(dplyr)
library(readr)
library(ggplot2)
#ne faite pas attention à ces lignes de code
#elles servent à définir automatiquement votre
#répertoire de travail dans le bon dossier
set_wd <- function() {
library(rstudioapi) # à installer au besoin
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path ))
print( getwd() )
}
set_wd()
#On importe et visualise le fichier csv issu des scraping
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(voix[which(elu=="oui")]),
difference = voix - voix_last_elu) %>%
filter(elu=="non", difference==-2)
library(dplyr)
library(readr)
library(ggplot2)
#ne faite pas attention à ces lignes de code
#elles servent à définir automatiquement votre
#répertoire de travail dans le bon dossier
set_wd <- function() {
library(rstudioapi) # à installer au besoin
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path ))
print( getwd() )
}
set_wd()
#On importe et visualise le fichier csv issu des scraping
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(Votes[which(elu=="oui")]),
difference = Votes - voix_last_elu) %>%
filter(elu=="non", difference==-2)
View(rate_lelection)
library(dplyr)
library(readr)
library(ggplot2)
#ne faite pas attention à ces lignes de code
#elles servent à définir automatiquement votre
#répertoire de travail dans le bon dossier
set_wd <- function() {
library(rstudioapi) # à installer au besoin
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path ))
print( getwd() )
}
set_wd()
#On importe et visualise le fichier csv issu des scraping
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(Votes[which(elu=="oui")]),
difference = Votes - voix_last_elu) %>%
filter(elu=="non", difference==-1)
View(rate_lelection)
library(dplyr)
library(readr)
library(ggplot2)
#ne faite pas attention à ces lignes de code
#elles servent à définir automatiquement votre
#répertoire de travail dans le bon dossier
set_wd <- function() {
library(rstudioapi) # à installer au besoin
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path ))
print( getwd() )
}
set_wd()
#On importe et visualise le fichier csv issu des scraping
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(Votes[which(elu=="oui")]),
difference = Votes - voix_last_elu) %>%
filter(elu=="non", difference==-3)
View(rate_lelection)
library(dplyr)
library(readr)
library(ggplot2)
#ne faite pas attention à ces lignes de code
#elles servent à définir automatiquement votre
#répertoire de travail dans le bon dossier
set_wd <- function() {
library(rstudioapi) # à installer au besoin
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path ))
print( getwd() )
}
set_wd()
#On importe et visualise le fichier csv issu des scraping
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(Votes[which(elu=="oui")]),
difference = Votes - voix_last_elu) %>%
filter(elu=="non", difference==-4)
library(dplyr)
library(readr)
library(ggplot2)
#ne faite pas attention à ces lignes de code
#elles servent à définir automatiquement votre
#répertoire de travail dans le bon dossier
set_wd <- function() {
library(rstudioapi) # à installer au besoin
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path ))
print( getwd() )
}
set_wd()
#On importe et visualise le fichier csv issu des scraping
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(Votes[which(elu=="oui")]),
difference = Votes - voix_last_elu) %>%
filter(elu=="non")
View(rate_lelection)
#On importe et visualise le fichier csv issu des scraping
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
library(readr)
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
library(dplyr)
library(readr)
library(ggplot2)
#On importe et visualise le fichier csv issu des scraping
library(readr)
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(Votes[which(elu=="oui")]),
difference = Votes - voix_last_elu) %>%
filter(elu=="non", difference==-2)
View(rate_lelection)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(Votes[which(elu=="oui")]),
difference = Votes - voix_last_elu) %>%
filter(elu=="non") %>%
arrange(desc(difference))
View(rate_lelection)
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(Votes[which(elu=="oui")]),
difference = Votes - voix_last_elu)
View(rate_lelection)
library(dplyr)
library(readr)
library(ggplot2)
#On importe et visualise le fichier csv issu des scraping
library(readr)
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(Votes[which(elu=="oui")]),
difference = Votes - voix_last_elu) %>%
filter(elu=="non") %>%
arrange(desc(difference))
library(dplyr)
library(readr)
library(ggplot2)
#On importe et visualise le fichier csv issu des scraping
library(readr)
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(Votes[which(elu=="oui")]),
difference = Votes - voix_last_elu) %>%
filter(elu=="non", difference==-2)
View(rate_lelection)
library(dplyr)
library(readr)
library(ggplot2)
#On importe et visualise le fichier csv issu des scraping
library(readr)
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(Votes[which(elu=="oui")]),
difference = Votes - voix_last_elu) %>%
filter(elu=="non", difference==-1)
View(rate_lelection)
library(dplyr)
library(readr)
library(ggplot2)
#On importe et visualise le fichier csv issu des scraping
library(readr)
data <- read_csv("C:/Users/ettor/Desktop/scraping_2018_wallonie_bruxelles_communales.csv")
View(scraping_2018_wallonie_bruxelles_communales)
#ajouter une colonne avec le nombre de voix du dernier élu
#calculer la différence entre cette colonne et le nombre de voix des premiers suppléants
rate_lelection =
data %>%
group_by(liste) %>%
mutate(voix_last_elu = min(Votes[which(elu=="oui")]),
difference = Votes - voix_last_elu) %>%
filter(elu=="non", difference==0)
View(rate_lelection)
