html_nodes("#wrn_background_Id") %>%
html_table(header = FALSE)
listes[1] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table(header = FALSE, fill=TRUE)
listes[1] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table(header = FALSE, fill=TRUE, dec=",")
listes[2] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
listes[3] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
listes[500] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
listes[1:100] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
for (url in listes[0:10]) {
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
}
for (url in listes[0:10]) {
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
}
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
}
table
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
tables = rbind(tables, table)
}
tables = vector()
tables = vector()
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
tables = rbind(tables, table)
}
tables
View(tables)
tables = vector()
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
tables = rbind(table, tables)
}
tables
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
}
table
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
}
table
tables_jointes = vector()
tables_jointes = vector()
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
tables_jointes = rbind(tables_jointes, table)
}
library(rvest)
library(readr)
#On importe la liste de liens récupérés à l'aide de web scraper chrome
liens_listes_wallonie_bruxelles <-
read_csv(
"C:/Users/ettor/Desktop/AJPRO_dataj/1_scraping/data/liens_listes_wallonie_bruxelles.csv"
)
#on jette un oeil sur le dataframe
View(liens_listes_wallonie_bruxelles)
##############################################################################
listes = liens_listes_wallonie_bruxelles$liens
###############################################################################
tables_jointes = vector()
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes
tables_jointes[0]
tables_jointes = vector()
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table(fill=TRUE)
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes[0]
tables_jointes
tables_jointes = vector()
for (url in listes[0:10]) {
html =
url %>% read_html()
table =
html %>%
html_nodes("#wrn_background_Id") %>%
html_table(fill=TRUE)
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes[0]
tables_jointes
listes[0:10]
tables_jointes = vector()
for (url in listes[0:10]) {
html = url %>%
read_html()
table = html %>%
html_node("#wrn_background_Id") %>%
html_table(header = FALSE,
fill = TRUE,
dec = ",")
#On n'oublie surtout pas de fusionner chaque table avec la liste tables
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes
View(tables_jointes)
tables_jointes = vector()
for (url in listes[0:10]) {
html =
url %>% read_html()
table =
html %>%
html_node("#wrn_background_Id") %>%
html_table(fill=TRUE)
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes
tables_jointes = vector()
for (url in listes[0:10]) {
html =
url %>% read_html()
table =
html %>%
html_node("#wrn_background_Id") %>%
html_table()
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes
tables_jointes = vector()
for (url in listes[100:120]) {
html =
url %>% read_html()
table =
html %>%
html_node("#wrn_background_Id") %>%
html_table()
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes
View(tables_jointes)
library(rvest)
library(readr)
#On importe la liste de liens récupérés à l'aide de web scraper chrome
liens_listes_wallonie_bruxelles <-
read_csv(
"C:/Users/ettor/Desktop/AJPRO_dataj/1_scraping/data/liens_listes_wallonie_bruxelles.csv"
)
#on jette un oeil sur le dataframe
View(liens_listes_wallonie_bruxelles)
##############################################################################
listes = liens_listes_wallonie_bruxelles$liens
###############################################################################
tables_jointes = vector()
for (url in listes[100:120]) {
html =
url %>% read_html()
table =
html %>%
html_node("#wrn_background_Id") %>%
html_table()
####################################################################
#on cree trois colonnes avec le nom de la commune, le nom de la liste et l'URL de la page
table$commune = html %>% html_node(".uppercaseb") %>% html_text()
table$parti = html %>% html_node(".subtitle") %>% html_text()
table$lien = i
#####################################################################
#On n'oublie surtout pas de fusionner chaque table avec la liste tables
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes
View(tables_jointes)
install.packages(c("callr", "car", "fs", "ggthemes", "sjPlot", "wordcloud"))
View(tables)
maListe = vector("ettore", "jean")
maListe = c("ettore", "jean")
maListe
len(maListe)
length(maListe)
maListe[1]
multiplieParDeux = function(nombre) {
nombre * 2
}
multiplieParDeux(4)
multiplieParDeux("ettore")
multiplieParDeux = function(nom) {
strrep(nom, 6)
}
multiplieParDeux("ettore")
multiplieParDeux = function(nom) {
strrep(nom, 6)
}
multiplieParDeux("ettore ")
multiplieParDeux = function(nom) {
#multiplie par deux
strrep(nom, 6)
}
multiplieParDeux("ettore ")
multiplie <- function(x,y) {
x * y
}
multiplie(4,5)
multiplie <- function(x,y,z) {
x * y - z
}
multiplie(4,5, 10)
library(rvest)
3 == 3
3 < 6
3 >= 7
monNom = "ettore"
monNom
toupper(monNom)
length(monNom)
nchar(monNom)
1+8
monNom = "Ettore Rizza"
monNom
ifelse(nchar(monNom)==6, "mon nom fait six lettres", NULL)
monNom = "Ettore Rizza"
ifelse(nchar(monNom)==6, "mon nom fait six lettres", NULL)
if (nchar(monNom)>5){
"votre nom a plus de 5 lettres"
}
if (nchar(monNom)>5){
"votre nom a plus de 5 lettres"
} else {"votre nom n'a pas 5 lettres"}
monNom = "jo"
if (nchar(monNom)>5){
"votre nom a plus de 5 lettres"
} else {"votre nom n'a pas 5 lettres"}
if (nchar(monNom)>5){
"votre nom a plus de 5 lettres"
}
else
{"votre nom n'a pas 5 lettres"}
if (nchar(monNom)>5){
"votre nom a plus de 5 lettres"
} else
{"votre nom n'a pas 5 lettres"}
for (i in monNom) {
i
}
print(i)
monNom = "ettore rizz"
for (i in monNom) {
print(i)
}
for (i in monNom) {
i
}
for (i in mesNoms) {
print(i)
}
mesNoms = c("ettore rizza", "jean", "albert")
for (i in mesNoms) {
print(i)
}
paste(i, "test")
for (i in mesNoms) {
paste(i, "test")
}
print(paste(i, "test"))
mesNoms = c("ettore rizza", "jean", "albert")
for (i in mesNoms) {
print(paste(i, "test"))
}
mesNoms = c("ettore rizza", "jean", "albert")
for (i in mesNoms) {
print(nchar(i))
}
for (noms in mesNoms) {
print(nchar(noms))
}
multiplieParDeux <- function(unNombre) {
unNombre * 2
}
multiplieParDeux(5)
multiplie <-
function(x, y) {
x * y
}
multiplie(5)
multiplie(5,6)
library(rvest)
read_html("https://stackoverflow.com/questions/40539981/loop-over-strings-in-r?rq=1")
read_html("https://stackoverflow.com/questions/40539981/loop-over-strings-in-r?rq=1")[1]
read_html("https://stackoverflow.com/questions/40539981/loop-over-strings-in-r?rq=1") %>% xml_structure()
read_html("https://stackoverflow.com/questions/40539981/loop-over-strings-in-r?rq=1") %>% html_structure()
read_html(mon_url)
mon_url = "https://stackoverflow.com/questions/40539981/loop-over-strings-in-r?rq=1"
read_html(mon_url)
monHtml = read_html(mon_url)
html_node(mon_html, "h1")
mon_url = "https://stackoverflow.com/questions/40539981/loop-over-strings-in-r?rq=1"
mon_html = read_html(mon_url)
html_node(mon_html, "h1")
html_text(h1)
h1 = html_node(mon_html, "h1")
html_text(h1)
texte_h1 = html_text(h1)
texte_h1
html_text(html_node(read_html(mon_url), "h1"))
html_text(h1)
mon_url %>%
read_html() %>%
html_node( "h1") %>%
html_text()
mon_h1 =
mon_url %>%
read_html() %>%
html_node( "h1") %>%
html_text()
mon_h1
install.packages("swirl")
library(swirl)
install_course("Getting_and_Cleaning_Data")
swirl()
library(dplyr)
cran = tbl_df(mydf)
install.packages("dbplyr")
data <- nycflights13::flights
data %>% select(year:day, dep_delay, arr_delay)
library(dbplyr)
data %>% select(year:day, dep_delay, arr_delay)
library(dplyr)
data %>% select(year:day, dep_delay, arr_delay)
data %>% select(year:day, dep_delay, arr_delay) %>% sql_render()
q = data %>% select(year:day, dep_delay, arr_delay) %>% sql_render()
q = data %>% select(year:day, dep_delay, arr_delay)
show_query(q)
100:200
install.packages(c("ape", "BMA", "bookdownplus", "chron", "cli", "condvis", "data.table", "digest", "doParallel", "emmeans", "esquisse", "eurostat", "expm", "ffbase", "flextable", "gbm", "ggcorrplot", "ggiraph", "ggridges", "ggvis", "h2o", "IRdisplay", "later", "leaflet", "magic", "maptools", "mapview", "MCMCpack", "mlr", "ndjson", "packcircles", "pkgbuild", "plotrix", "pROC", "PythonInR", "R.utils", "rapidjsonr", "rcmdcheck", "Rcmdr", "RcmdrMisc", "reprex", "robustbase", "RSelenium", "RtutoR", "rtweet", "RWeka", "RWekajars", "sessioninfo", "sjlabelled", "sjmisc", "sparklyr", "sparsepp", "spData", "spdep", "tinytex", "TTR", "tweenr", "units", "webshot", "xtable", "xts", "zoo"))
sqlquery:::sq_addin_app()
sqlquery:::sq_addin()
library(DBI)
con <- dbConnect(RSQLite::SQLite(), ":memory:")
dbWriteTable(conn = con, name = "mtcars", value = mtcars)
sql_query(conn = con)
library(sqlquery)
library(DBI)
con <- dbConnect(RSQLite::SQLite(), ":memory:")
dbWriteTable(conn = con, name = "mtcars", value = mtcars)
sql_query(conn = con)
sqlquery:::sq_addin_app()
library(sqlquery)
query <- "select * from mtcars"
addinexamples:::findAndReplaceAddin()
# Start the clock! Pour mesurer (par curiosité) le temps que prendra l'execution de ce script
start_time <- Sys.time()
#On importe les trois packages nécessaires (en les installant si nécessaire)
library(rvest) #le package de scraping
library(tidyverse) #contient tout ce qu'il faut pour manipuler les données
library(rstudioapi) #ne sert que pour la fonction setwd()
#ne faite pas attention aux six lignes de code suivantes.
#elles servent juste à définir automatiquement votre
#répertoire de travail dans le bon dossier,
#ce qui vous évitera des chipotages.
#Votre répértoire de travail (l'endroit où vous pourrez retrouver les résultats)
#sera affiché dans votre concole, en bas.
set_wd <- function() {
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path ))
print( getwd() )
}
set_wd()
#####################################################################
#ATTENTION, ces deux URL devront être modifiées le 14 octobre !
#URL de la page contenant les liens vers chaque commune (page de départ)
start_url = "https://rwa-ma5.martineproject.be/fr/election?el=CG"
#Url de base du site (sera différente pour la Wallonie et Bruxelles)
base_url = "https://rwa-ma5.martineproject.be"
#####################################################################
#On récupère les urls des communes
url_communes =
start_url %>%
read_html() %>%
#le sélecteur CSS "#block-blockelectionstart a" a été retrouvé (après un peu de chipo...)
#grâce au selector gadget de Chrome
html_nodes("#block-blockelectionstart a") %>%
html_attr("href") %>%
paste0(base_url, .)
#On récupère les URLs des listes électorales de chaque commune
listes = vector()
for (url in url_communes) {
urls_listes = url %>%
read_html() %>%
html_nodes(".text-center a") %>%
html_attr("href") %>%
gsub("../preferred/", "/preferred/", .) %>%
paste0(base_url, .)
listes = c(listes, urls_listes)
}
#On récupère les tableaux de résultats qu'on stocke dans une seule liste
tables_completes = vector()
for (urls_tables in listes) {
html = urls_tables %>%
read_html()
block_table = html %>%
html_table(header = TRUE,
fill = TRUE,
dec = ",")
table = block_table[[3]]
#on crée trois nouvelles colonnes pour stocker le nom de la commune, le nom de la liste et l'URL de la page
table$commune = html %>% html_node(".article-dataheader__title") %>% html_text()
table$parti = html %>% html_node(".w-100") %>% html_text()
table$lien = urls_tables
#on fusionne chaque nouvelle table dans un tableau global
tables_completes = rbind(tables_completes, table)
}
#########################################################################
#La partie qui suit est du nettoyage de données. Elle pourrait très bien
#être effectuée dans OpenRefine, voire dans Excel. Mais ceci vous fera gagner du temps.
#########################################################################
#On met les poucentages sous forme de nombres
tables_completes$pourcent_liste = as.numeric(gsub("^([0-9]+),?([0-9]+)?%", "\\1.\\2", tables_completes$pourcent_liste))
#on élimine les mots "Commune de" et on efface les éventuels espaces en trop
tables_completes$commune = gsub("Commune de ", "", trimws(tables_completes$commune))
#on enlève le numéro devant le nom de parti et on efface les éventuels espaces en trop
tables_completes$parti = gsub("^\\d+ ", "", trimws(tables_completes$parti))
#on crée une colonne "liste" en concaténant (collant) nom de commune et de parti
tables_completes$liste = paste(tables_completes$commune, tables_completes$parti, sep="-")
# Stop the clock : affichera dans la console le temps d'execution du script
#(environ 15 minutes pour le site de test wallon, son serveur étant lent)
Sys.time() - start_time
#On sauvegarde les résultats dans un csv "Excel compatible"
write_excel_csv(tables_completes, "scraping_2018_test.csv")
usePackage <- function(p)
{
if (!is.element(p, installed.packages()[,1]))
install.packages(p, dep = TRUE)
require(p, character.only = TRUE)
}
usePackage(rvest) #le package de scraping
usePackage(tidyverse) #contient tout ce qu'il faut pour manipuler les données
usePackage(rstudioapi) #ne sert que pour la fonction setwd()
usePackage(test)
usePackage <- function(p)
{
if (!is.element(p, installed.packages()[,1]))
install.packages(p, dep = TRUE)
require(p, character.only = TRUE)
}
usePackage("rvest") #le package de scraping
usePackage("tidyverse") #contient tout ce qu'il faut pour manipuler les données
usePackage("rstudioapi") #ne sert que pour la fonction setwd()
usePackage("test")
usePackage <- function(p)
{
if (!is.element(p, installed.packages()[,1]))
install.packages(p, dep = TRUE)
require(p, character.only = TRUE)
}
usePackage("rvest") #le package de scraping
usePackage("tidyverse") #contient tout ce qu'il faut pour manipuler les données
usePackage("rstudioapi") #ne sert que pour la fonction setwd()
